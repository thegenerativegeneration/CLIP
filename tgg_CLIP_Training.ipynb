{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegenerativegeneration/CLIP/blob/main/tgg_CLIP_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmNBYypq2lId"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivSj8FDWJfru"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHWCcYLG2PnS"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)\n",
        "\n",
        "\n",
        "import subprocess, os, sys, ipykernel\n",
        "\n",
        "def gitclone(url, targetdir=None):\n",
        "    if targetdir:\n",
        "        res = subprocess.run(['git', 'clone', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "        res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Google Colab detected. Using Google Drive.\")\n",
        "    is_colab = True\n",
        "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
        "    google_drive = True #@param {type:\"boolean\"}\n",
        "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
        "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "except:\n",
        "    is_colab = False\n",
        "    google_drive = False\n",
        "    save_models_to_google_drive = False\n",
        "    print(\"Google Colab not detected.\")\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        drive.mount('/content/drive')\n",
        "        root_path = '/content/drive/MyDrive/AI/Disco_Diffusion'\n",
        "    else:\n",
        "        root_path = '/content'\n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "\n",
        "import os\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "\n",
        "import pathlib, shutil, os, sys\n",
        "\n",
        "# There are some reports that with a T4 or V100 on Colab, downgrading to a previous version of PyTorch may be necessary.\n",
        "# .. but there are also reports that downgrading breaks them!  If you're facing issues, you may want to try uncommenting and running this code.\n",
        "# nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "# cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
        "# if is_colab:\n",
        "#     if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
        "#         print(\"Downgrading pytorch. This can take a couple minutes ...\")\n",
        "#         downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "#         print(\"pytorch downgraded.\")\n",
        "\n",
        "#@markdown Check this if you want to use CPU\n",
        "useCPU = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not is_colab:\n",
        "    # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
        "    os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
        "\n",
        "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
        "USE_ADABINS = True\n",
        "\n",
        "if is_colab:\n",
        "    if not google_drive:\n",
        "        root_path = f'/content'\n",
        "        model_path = '/content/models' \n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "    model_path = f'{root_path}/models'\n",
        "\n",
        "multipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(multipip_res)\n",
        "\n",
        "if is_colab:\n",
        "    subprocess.run(['apt', 'install', 'imagemagick'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "try:\n",
        "    from CLIP import clip\n",
        "except:\n",
        "    if not os.path.exists(\"CLIP\"):\n",
        "        gitclone(\"https://github.com/openai/CLIP\")\n",
        "    sys.path.append(f'{PROJECT_DIR}/CLIP')\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import timm\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "from glob import glob\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from CLIP import clip\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from ipywidgets import Output\n",
        "import hashlib\n",
        "from functools import partial\n",
        "\n",
        "from IPython.display import Image as ipyimg\n",
        "from numpy import asarray\n",
        "from einops import rearrange, repeat\n",
        "import torch, torchvision\n",
        "import time\n",
        "from omegaconf import OmegaConf\n",
        "import warnings\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if (torch.cuda.is_available() and not useCPU) else 'cpu')\n",
        "print('Using device:', DEVICE)\n",
        "device = DEVICE # At least one of the modules expects this name..\n",
        "\n",
        "\n",
        "if not useCPU:\n",
        "    if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
        "        print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
        "        torch.backends.cudnn.enabled = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kZLy-Zygv3G"
      },
      "outputs": [],
      "source": [
        "!cd /content && unzip /content/drive/MyDrive/data/clip_data_v9.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiF2egWq3umV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "\n",
        "\n",
        "dataset_root = \"/content/clip_data\"#@param\n",
        "checkpoint_path = \"/content/drive/MyDrive/AI/clip_checkpoints_v9\"#@param\n",
        "\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 128#@param\n",
        "EPOCH = 100#@param\n",
        "model_suffix = \"_ukiyoe\"#@param\n",
        "clip_model_name = \"ViT-B/32\"#@param [\"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\", \"RN50\", \"RN50x4\", \"RN50x16\", \"RN50x64\", \"RN101\", \"ViT-L/14@336px\"]\n",
        "model_name = clip_model_name.replace('/', '') + model_suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLDv0vWKQ99r"
      },
      "outputs": [],
      "source": [
        "\n",
        "model, preprocess = clip.load(clip_model_name,device=device,jit=False) #Must set jit=False for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl-KtwC2qcZk"
      },
      "outputs": [],
      "source": [
        "#@markdown Choose which layers you don't want to train. These layers will have it's gradient disabled and only unchecked layers will be trained. This corresponds to Vit-B/32 architecture.\n",
        "\n",
        "freeze_visual = False#@param{type: 'boolean'}\n",
        "freeze_token_embedding = True#@param{type: 'boolean'}\n",
        "freeze_transformer = False#@param{type: 'boolean'}\n",
        "freeze_ln_final = True#@param{type: 'boolean'}\n",
        "for child in model.named_children():\n",
        "  if freeze_visual and child[0] == \"visual\":\n",
        "    print(f\"Freezing: {child[0]}\")\n",
        "    for param in child[1].parameters():\n",
        "      param.requires_grad = False\n",
        "  if freeze_token_embedding and child[0] == \"token_embedding\":\n",
        "    print(f\"Freezing: {child[0]}\")\n",
        "    for param in child[1].parameters():\n",
        "      param.requires_grad = False\n",
        "  if freeze_transformer and child[0] == \"transformer\":\n",
        "    print(f\"Freezing: {child[0]}\")\n",
        "    for param in child[1].parameters():\n",
        "      param.requires_grad = False\n",
        "  if freeze_ln_final and child[0] == \"ln_final\":\n",
        "    print(f\"Freezing: {child[0]}\")\n",
        "    for param in child[1].parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbW7spg2mLK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qCJDCDnxuurt"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, RandomResizedCrop\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def _transform(n_px):\n",
        "    return Compose([\n",
        "        RandomResizedCrop(n_px, scale=(0.8,1.0), ratio=(1.0,1.0), interpolation=BICUBIC),\n",
        "        RandomHorizontalFlip(0.5),\n",
        "        _convert_image_to_rgb,\n",
        "        ToTensor(),\n",
        "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "    ])\n",
        "\n",
        "class image_title_dataset(Dataset):\n",
        "    def __init__(self, list_image_path,list_txt, transforms):\n",
        "\n",
        "        self.image_path = list_image_path\n",
        "        self.title  = clip.tokenize(list_txt, truncate=True) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_path[idx])\n",
        "        image = self.transforms(image) # Image from PIL module\n",
        "        title = self.title[idx]\n",
        "        return image,title\n",
        "\n",
        "# use your own data\n",
        "list_image_path = [] \n",
        "list_txt = []\n",
        "\n",
        "print(\"Loading dataset\")\n",
        "\n",
        "source_images_path = f\"{dataset_root}/images\"\n",
        "files = [f for f in os.listdir(source_images_path) if os.path.isfile(f\"{source_images_path}/{f}\") and (f.endswith(\".png\") or f.endswith(\".jpg\") or f.endswith(\".jpeg\") or f.endswith(\".webp\"))]\n",
        "\n",
        "ttl = len(files)\n",
        "out = display(progress(0, ttl), display_id=True)\n",
        "ix = 0\n",
        "for f in files:\n",
        "    out.update(progress(ix, ttl))\n",
        "    ix+= 1\n",
        "    im_data_number = f.split('.')[0]\n",
        "    if not os.path.isfile(f\"{dataset_root}/text/{im_data_number}.txt\"):\n",
        "        continue\n",
        "    with open(f\"{dataset_root}/text/{im_data_number}.txt\") as textfile:\n",
        "        lines = textfile.readlines()\n",
        "    for line in lines:\n",
        "      if line:\n",
        "        image_path = f\"{source_images_path}/{f}\"\n",
        "        list_image_path.append(image_path)\n",
        "        list_txt.append(line)\n",
        "    \n",
        "\n",
        "dataset = image_title_dataset(list_image_path, list_txt, _transform(model.visual.input_resolution))\n",
        "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE, shuffle=True) #Define your own dataloader\n",
        "\n",
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model): \n",
        "    for p in model.parameters(): \n",
        "        p.data = p.data.float() \n",
        "        if p.grad is not None:\n",
        "          p.grad.data = p.grad.data.float() \n",
        "\n",
        "\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.002) # reduced learning rate and weight decay - recommended by various papers\n",
        "\n",
        "# add your own code to track the training progress.\n",
        "print(\"Starting training\")\n",
        "for epoch in range(EPOCH+1):\n",
        "  print(f\"EPOCH {epoch}\")\n",
        "  epoch_loss = 0.0\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images,texts = batch \n",
        "    \n",
        "      images= images.to(device)\n",
        "      texts = texts.to(device)\n",
        "    \n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "      img_loss = loss_img(logits_per_image,ground_truth)\n",
        "      txt_loss= loss_txt(logits_per_text,ground_truth)\n",
        "\n",
        "      total_loss = ( img_loss+txt_loss )/2\n",
        "      total_loss.backward()\n",
        "\n",
        "      if device == \"cpu\":\n",
        "         optimizer.step()\n",
        "      else : \n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model)\n",
        "      epoch_loss += total_loss.item()\n",
        "      if i % 10 == 0:\n",
        "        print(f'image loss: {img_loss.item():>7f} text loss: {txt_loss.item():>7f} total loss: {total_loss.item()} batch: {i+1}')\n",
        "  print(f\" Epoch loss: {epoch_loss}\")\n",
        "  if epoch % 1 == 0:\n",
        "    print(f\"Saving checkpoint: {checkpoint_path}/{model_name}_e{epoch}.pt\")\n",
        "    torch.save(model.state_dict(), f\"{checkpoint_path}/{model_name}_e{epoch}.pt\")\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizers_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss,\n",
        "            }, f\"{checkpoint_path}/{model_name}_e{epoch}_full.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCBJ-OXmA5HC"
      },
      "outputs": [],
      "source": [
        "!cat /content/clip_data_v6/text/7603-49.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItjRasbmT7tY"
      },
      "source": [
        "# TODO: Wise-FT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DiqYIvnT6V_"
      },
      "outputs": [],
      "source": [
        "alpha = 0.75\n",
        "#finetuned_model = model\n",
        "\n",
        "epoch = 20\n",
        "finetuned_model_statedict = torch.load(f\"/content/drive/MyDrive/AI/clip_checkpoints_v9/ViT-B32_ukiyoe_e{epoch}.pt\", map_location=device) \n",
        "zeroshot_model, preprocess = clip.load(clip_model_name,device=device,jit=False) \n",
        "\n",
        "theta_0 = zeroshot_model.state_dict()\n",
        "theta_1 = finetuned_model_statedict\n",
        "\n",
        "# make sure checkpoints are compatible\n",
        "assert set(theta_0.keys()) == set(theta_1.keys())\n",
        "\n",
        "\n",
        "# interpolate between checkpoints with mixing coefficient alpha\n",
        "theta = {\n",
        "    key: (1-alpha) * theta_0[key] + alpha * theta_1[key]\n",
        "    for key in theta_0.keys()\n",
        "}\n",
        "\n",
        "# update the model acccording to the new weights\n",
        "#finetuned_model.load_state_dict(theta)\n",
        "\n",
        "\n",
        "torch.save(theta, f\"{checkpoint_path}/{model_name}_e{epoch}-wiseft_alpha-{alpha}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6agihQ26gEB"
      },
      "outputs": [],
      "source": [
        "print(finetuned_model.__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYzk6-4BLv8w"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "tgg CLIP_Training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}